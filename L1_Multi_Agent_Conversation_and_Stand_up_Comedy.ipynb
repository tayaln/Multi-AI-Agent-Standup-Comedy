{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a81456dd",
      "metadata": {
        "id": "a81456dd"
      },
      "source": [
        "# Lesson 1: Multi-Agent Conversation and Stand-up Comedy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sKEE5NEiJnGb"
      },
      "id": "sKEE5NEiJnGb"
    },
    {
      "cell_type": "markdown",
      "id": "742cf649",
      "metadata": {
        "id": "742cf649"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyautogen==0.2.25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z65YxoZ4EQ5k",
        "outputId": "c62703c9-febb-49ae-b73f-a901d4af5832"
      },
      "id": "Z65YxoZ4EQ5k",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/257.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m256.0/257.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.1/257.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.2/314.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S8DJoxJdj3l",
        "outputId": "299d3a60-1601-4d89-f535-fffcca68b427"
      },
      "id": "6S8DJoxJdj3l",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.0/457.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "04d006c1-22fa-40ea-b3e0-d543142e0788",
      "metadata": {
        "height": 64,
        "id": "04d006c1-22fa-40ea-b3e0-d543142e0788"
      },
      "outputs": [],
      "source": [
        "#from utils import get_openai_api_key\n",
        "#OPENAI_API_KEY = get_openai_api_key()\n",
        "\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPEN_API_KEY')\n",
        "\n",
        "llm_config = {\"model\": \"gpt-3.5-turbo\"}"
      ],
      "metadata": {
        "id": "vGH1AEEhegcG"
      },
      "id": "vGH1AEEhegcG",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "116a1c4d",
      "metadata": {
        "id": "116a1c4d"
      },
      "source": [
        "## Define an AutoGen agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6fb8c441-c58c-41a8-a54b-5c387afceac5",
      "metadata": {
        "height": 132,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb8c441-c58c-41a8-a54b-5c387afceac5",
        "outputId": "18852e4e-3b28-497a-e660-07d1b1369256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 12-18 10:49:07] {129} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ],
      "source": [
        "from autogen import ConversableAgent\n",
        "\n",
        "agent = ConversableAgent(\n",
        "    name=\"chatbot\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "47886b5f-fc7c-431a-8036-cff6e88f85c6",
      "metadata": {
        "height": 98,
        "id": "47886b5f-fc7c-431a-8036-cff6e88f85c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58758b0c-00a1-4be0-85a1-73ecec7c9e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a punny joke for you:\n",
            "\n",
            "Why did the bicycle fall over?\n",
            "\n",
            "Because it was two-tired!\n"
          ]
        }
      ],
      "source": [
        "reply = agent.generate_reply(\n",
        "    messages=[{\"content\": \"Tell me a PJ.\", \"role\": \"user\"}]\n",
        ")\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c98a301",
      "metadata": {
        "id": "8c98a301"
      },
      "source": [
        "## Conversation\n",
        "\n",
        "Setting up a conversation between two agents, Jai and Veeru, where the memory of their interactions is retained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8f109dcb-824e-40d7-8e86-efee42b75f3c",
      "metadata": {
        "height": 302,
        "id": "8f109dcb-824e-40d7-8e86-efee42b75f3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb717517-e127-4c43-9b3c-45758295b9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 12-18 10:50:46] {129} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 12-18 10:50:46] {129} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ],
      "source": [
        "Jai = ConversableAgent(\n",
        "    name=\"Jai\",\n",
        "    system_message=\n",
        "    \"Your name is Jai  and you are a stand-up comedian known for your PJs.\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "Veeru = ConversableAgent(\n",
        "    name=\"Veeru\",\n",
        "    system_message=\n",
        "    \"Your name is Veeru and you are a stand-up comedian famous for your PJs. \"\n",
        "    \"Start the next joke from the punchline of the previous joke.\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43f71a61",
      "metadata": {
        "id": "43f71a61"
      },
      "source": [
        "**Note**: You might get a slightly different response (set of jokes) than what is shown in the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "46a1c6f6-687e-40de-8819-374201cfed9f",
      "metadata": {
        "height": 115,
        "id": "46a1c6f6-687e-40de-8819-374201cfed9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d81532-35b9-4aff-cc2d-73ec89cc0873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veeru (to Jai):\n",
            "\n",
            "I'm Veeru. Jai, let's keep the PJs rolling.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Jai (to Veeru):\n",
            "\n",
            "Hey Veeru, why did the math book look sad? Because it had too many problems!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Veeru (to Jai):\n",
            "\n",
            "Why did the tomato turn red? Because it saw the salad dressing!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Jai (to Veeru):\n",
            "\n",
            "Haha, that's a good one, Veeru! Speaking of tomatoes, why did the tomato turn to the corner? Because it saw the salad dressing!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Veeru (to Jai):\n",
            "\n",
            "Why did the belt get arrested? Because it held up a pair of pants!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Jai (to Veeru):\n",
            "\n",
            "Oh, Veeru, that one really pulled through! Why don't skeletons fight each other? They don't have the guts!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chat_result = Veeru.initiate_chat(\n",
        "    recipient=Jai,\n",
        "    message=\"I'm Veeru. Jai, let's keep the PJs rolling.\",\n",
        "    max_turns=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78edc810",
      "metadata": {
        "id": "78edc810"
      },
      "source": [
        "## Print some results\n",
        "\n",
        "You can print out:\n",
        "\n",
        "1. Chat history\n",
        "2. Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1169ea24-eadd-4909-8d56-9b7ec5677c66",
      "metadata": {
        "height": 64,
        "id": "1169ea24-eadd-4909-8d56-9b7ec5677c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7edfba-ec54-466e-a981-726d1c8c86ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'content': \"I'm Veeru. Jai, let's keep the PJs rolling.\",\n",
            "  'role': 'assistant'},\n",
            " {'content': 'Hey Veeru, why did the math book look sad? Because it had too '\n",
            "             'many problems!',\n",
            "  'role': 'user'},\n",
            " {'content': 'Why did the tomato turn red? Because it saw the salad dressing!',\n",
            "  'role': 'assistant'},\n",
            " {'content': \"Haha, that's a good one, Veeru! Speaking of tomatoes, why did \"\n",
            "             'the tomato turn to the corner? Because it saw the salad '\n",
            "             'dressing!',\n",
            "  'role': 'user'},\n",
            " {'content': 'Why did the belt get arrested? Because it held up a pair of '\n",
            "             'pants!',\n",
            "  'role': 'assistant'},\n",
            " {'content': \"Oh, Veeru, that one really pulled through! Why don't skeletons \"\n",
            "             \"fight each other? They don't have the guts!\",\n",
            "  'role': 'user'}]\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(chat_result.chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "550267b6-3652-40dc-9997-c5401f6d4c47",
      "metadata": {
        "height": 30,
        "id": "550267b6-3652-40dc-9997-c5401f6d4c47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf2575a-99dc-4be3-f7d6-1c918697df52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'usage_excluding_cached_inference': {'gpt-3.5-turbo-0125': {'completion_tokens': 110,\n",
            "                                                             'cost': 0.0004175,\n",
            "                                                             'prompt_tokens': 505,\n",
            "                                                             'total_tokens': 615},\n",
            "                                      'total_cost': 0.0004175},\n",
            " 'usage_including_cached_inference': {'gpt-3.5-turbo-0125': {'completion_tokens': 110,\n",
            "                                                             'cost': 0.0004175,\n",
            "                                                             'prompt_tokens': 505,\n",
            "                                                             'total_tokens': 615},\n",
            "                                      'total_cost': 0.0004175}}\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(chat_result.cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "300525bd",
      "metadata": {
        "id": "300525bd"
      },
      "source": [
        "## Chat Termination\n",
        "\n",
        "Chat can be terminated using a termination conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "044dfd61-7f1d-46d8-9e28-4b2601b43d70",
      "metadata": {
        "height": 353,
        "id": "044dfd61-7f1d-46d8-9e28-4b2601b43d70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543ce5f8-ff10-4763-cc2b-7143613ddabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 12-18 10:54:46] {129} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 12-18 10:54:46] {129} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ],
      "source": [
        "Jai = ConversableAgent(\n",
        "    name=\"Jai\",\n",
        "    system_message=\n",
        "    \"Your name is Jai and you are a stand-up comedian. \"\n",
        "    \"When you're ready to end the conversation, say 'I gotta go' or 'Good Bye'.\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda msg: \"I gotta go\" in msg[\"content\"] or \"Goodbye\" in msg[\"content\"],\n",
        ")\n",
        "\n",
        "Veeru = ConversableAgent(\n",
        "    name=\"Veeru\",\n",
        "    system_message=\n",
        "    \"Your name is Veeru and you are a stand-up comedian. \"\n",
        "    \"When you're ready to end the conversation, say 'I gotta go' or 'Good Bye'.\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda msg: \"I gotta go\" in msg[\"content\"] or \"Goodbye\" in msg[\"content\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bc49d959-1025-4709-8866-9d4035eaeae7",
      "metadata": {
        "height": 98,
        "id": "bc49d959-1025-4709-8866-9d4035eaeae7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0781f7d2-7589-4c91-bbf7-03e6f39e276d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veeru (to Jai):\n",
            "\n",
            "I'm Veeru. Jai, let's keep the jokes rolling.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Jai (to Veeru):\n",
            "\n",
            "Hey Veeru, great to have you here! Let me start with a classic one for you. Why couldn't the bicycle find its way home? Because it lost its bearings!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Veeru (to Jai):\n",
            "\n",
            "Haha, good one! That bicycle needs a GPS for sure. How about this one: Why did the math book look sad? Because it had too many problems.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Jai (to Veeru):\n",
            "\n",
            "Haha, I love that one, Veeru! It seems like that math book could use a break and maybe a little therapy. Here's another one for you: Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Veeru (to Jai):\n",
            "\n",
            "Haha, that scarecrow must have really stood out among the cornstalks! Here's one for you: Why don't scientists trust atoms? Because they make up everything!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Jai (to Veeru):\n",
            "\n",
            "Haha, that's a classic one, Veeru! Scientists and their skepticism about those sneaky little atoms, always up to something. How about this one: Why did the tomato turn red? Because it saw the salad dressing!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Veeru (to Jai):\n",
            "\n",
            "Haha, that's a saucy joke! I bet that tomato didn't ketchup to the salad dressing. You're on a roll!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Jai (to Veeru):\n",
            "\n",
            "Thanks, Veeru! I'm glad you're enjoying the jokes. It's always fun to have a good laugh. If you ever need a joke to brighten your day, you know where to find me. Laughter is the best medicine, after all!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Veeru (to Jai):\n",
            "\n",
            "Absolutely, laughter is the best medicine indeed! Thanks for the jokes, they really brought a smile to my face. Feel free to drop by anytime for a good laugh. Take care now!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Jai (to Veeru):\n",
            "\n",
            "You're welcome, Veeru! I'm glad I could bring a smile to your face. Take care and remember, keep laughing and spreading the joy! Goodbye!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chat_result = Veeru.initiate_chat(\n",
        "    recipient=Jai,\n",
        "    message=\"I'm Veeru. Jai, let's keep the jokes rolling.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}